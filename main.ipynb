{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a481eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from einops.layers.torch import Rearrange\n",
    "from tqdm.notebook import tqdm\n",
    "from termcolor import cprint\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ccc1bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1248e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習、検証、テスト用のデータセットの定義\n",
    "\n",
    "class ThingsEEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str):\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"val\", \"test\"], f\"Invalid split: {split}\"\n",
    "        \n",
    "        self.X = torch.from_numpy(np.load(f\"data/{split}/eeg.npy\")).to(torch.float32)\n",
    "        self.subject_idxs = torch.from_numpy(np.load(f\"data/{split}/subject_idxs.npy\"))\n",
    "\n",
    "        if split in [\"train\", \"val\"]:\n",
    "            self.y = torch.from_numpy(np.load(f\"data/{split}/labels.npy\"))\n",
    "        else:\n",
    "            self.y = None # testセットにはラベルがない\n",
    "\n",
    "        print(f\"[{split.upper()} SET] EEG: {self.X.shape}, Subject Indices: {self.subject_idxs.shape}\", end=\"\")\n",
    "        if self.y is not None:\n",
    "            print(f\", Labels: {self.y.shape}\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.y is not None:\n",
    "            return self.X[i], self.y[i], self.subject_idxs[i]\n",
    "        else:\n",
    "            return self.X[i], self.subject_idxs[i]\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return 5 # animal, food, clothing, tool, vehicle\n",
    "\n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2066f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 脳波データとそれに対応する画像の両方を読み込むデータセットの定義\n",
    "\n",
    "class ImageEEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str, image_dir: str = \"training_images\", subset_size: int = None):\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"val\"], f\"Invalid split: {split}\"\n",
    "\n",
    "        self.X_eeg_full = torch.from_numpy(np.load(f\"data/{split}/eeg.npy\")).to(torch.float32)\n",
    "\n",
    "        project_root = os.getcwd()\n",
    "        print(f\"Project root detected as: {project_root}\")\n",
    "\n",
    "        self.image_dir_abs = os.path.join(project_root, image_dir)\n",
    "        if not os.path.isdir(self.image_dir_abs):\n",
    "            raise FileNotFoundError(f\"Image directory not found: {self.image_dir_abs}\")\n",
    "\n",
    "        with open(f\"data/{split}/image_paths.txt\", \"r\") as f:\n",
    "            self.image_paths_full = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        if subset_size is not None:\n",
    "            print(f\"Using a subset of {subset_size} samples from the full dataset.\")\n",
    "            self.X_eeg = self.X_eeg_full[:subset_size]\n",
    "            self.image_paths = self.image_paths_full[:subset_size]\n",
    "        else:\n",
    "            print(\"Using the full dataset.\")\n",
    "            self.X_eeg = self.X_eeg_full\n",
    "            self.image_paths = self.image_paths_full\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        # 画像の前処理\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_eeg)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        eeg_data = self.X_eeg[i]\n",
    "        \n",
    "        image_path = os.path.join(self.image_dir, self.image_paths[i])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_data = self.transform(image)\n",
    "        \n",
    "        return eeg_data, image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a1e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size: int = 3, p_drop: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        self.conv1 = nn.Conv1d(out_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        self.batchnorm0 = nn.BatchNorm1d(num_features=out_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=out_dim)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if self.in_dim == self.out_dim:\n",
    "            X_skip = X\n",
    "            X = self.conv0(X)\n",
    "            X = X + X_skip # スキップ接続\n",
    "        else:\n",
    "            X = self.conv0(X)\n",
    "\n",
    "        X = F.gelu(self.batchnorm0(X))\n",
    "        \n",
    "        X_skip = X\n",
    "        X = self.conv1(X)\n",
    "        X = X + X_skip # スキップ接続\n",
    "        X = F.gelu(self.batchnorm1(X))\n",
    "\n",
    "        return self.dropout(X)\n",
    "\n",
    "class ConvRNNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels: int, hid_dim: int = 128, rnn_layers: int = 2, embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN Part: 局所的な特徴を抽出\n",
    "        self.cnn = nn.Sequential(\n",
    "            ConvBlock(in_channels, hid_dim),\n",
    "            nn.MaxPool1d(2),\n",
    "            ConvBlock(hid_dim, hid_dim),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "\n",
    "        # RNN Part: 時系列の長期的な依存関係を捉える\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=hid_dim,\n",
    "            hidden_size=embedding_dim,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim * 2 * rnn_layers, embedding_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, in_channels, seq_len)\n",
    "        x = self.cnn(x) # (batch_size, hid_dim, seq_len / 4)\n",
    "        x = x.permute(0, 2, 1) # (battch_size, seq_len / 4, hid_dim)\n",
    "\n",
    "        _, (h_n, _) = self.rnn(x) # h_n: (num_layers * 2, batch_size, embeding_dim)\n",
    "        h_n = h_n.permute(1, 0, 2).contiguous() # (batch_size, num_layers*2, embedding_dim)\n",
    "        h_n = h_n.view(h_n.size(0), -1) # batch_size, num_layers*2*embedding_dim)\n",
    "\n",
    "        embedding = self.fc(h_n)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1099a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像と脳波をエンコードし、埋め込みベクトルを出力するモデル\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, eeg_encoder: nn.Module, embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.eeg_encoder = eeg_encoder\n",
    "\n",
    "        # 画像エンコーダーとして事前学習済みのResNetを使用\n",
    "        self.image_encoder = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        # ResNetの最終層を付け替えて、埋め込みベクトルを出力するようにする\n",
    "        num_ftrs = self.image_encoder.fc.in_features\n",
    "        self.image_encoder.fc = nn.Linear(num_ftrs, embedding_dim)\n",
    "    \n",
    "    def forward(self, eeg, image):\n",
    "        eeg_embedding = self.eeg_encoder(eeg)\n",
    "        image_embedding = self.image_encoder(image)\n",
    "        return eeg_embedding, image_embedding\n",
    "\n",
    "# 対照学習損失 (InfoNCE Loss)\n",
    "def contrastive_loss(eeg_embeds, img_embeds, temperature=0.07):\n",
    "    # L2正規化\n",
    "    eeg_embeds = F.normalize(eeg_embeds, p=2, dim=-1)\n",
    "    img_embeds = F.normalize(img_embeds, p=2, dim=-1)\n",
    "    \n",
    "    # コサイン類似度行列を計算\n",
    "    logits = torch.matmul(eeg_embeds, img_embeds.T) / temperature\n",
    "    \n",
    "    # 正解ラベルは対角成分\n",
    "    labels = torch.arange(len(logits)).to(logits.device)\n",
    "    \n",
    "    # EEG->Image と Image->EEG の両方向で損失を計算\n",
    "    loss_eeg = F.cross_entropy(logits, labels)\n",
    "    loss_img = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_eeg + loss_img) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root detected as: /Users/odajohan/DL基礎講座/Deep-Learning-Basic-2025-\n",
      "Using a subset of 30000 samples from the full dataset.\n",
      "Pretraining model parameters set to not require gradients.\n",
      "EEG encoder parameters are explicitly set to be trainable.\n"
     ]
    }
   ],
   "source": [
    "# --- 事前学習の準備 ---\n",
    "pretrain_epochs = 20 \n",
    "pretrain_lr = 1e-4\n",
    "embedding_dim = 256\n",
    "subset_data_size = 30000\n",
    "\n",
    "pretrain_dataset = ImageEEGDataset(\"train\", subset_size=subset_data_size)\n",
    "pretrain_loader = DataLoader(pretrain_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "eeg_encoder = ConvRNNEncoder(in_channels=17, embedding_dim=embedding_dim).to(device)\n",
    "pretrain_model = MultiModalModel(eeg_encoder, embedding_dim=embedding_dim).to(device)\n",
    "\n",
    "for param in pretrain_model.parameters():\n",
    "    param.requires_grad = False # prameterを凍結して計算負荷を軽減\n",
    "print(\"Pretraining model parameters set to not require gradients.\")\n",
    "\n",
    "for param in pretrain_model.eeg_encoder.parameters():\n",
    "    param.requires_grad = True # EEG encoder parameters は学習対象\n",
    "print(\"EEG encoder parameters are explicitly set to be trainable.\")\n",
    "\n",
    "# 最適化手法\n",
    "trainable_params = filter(lambda p: p.requires_grad, pretrain_model.parameters())\n",
    "pretrain_optimizer = torch.optim.Adam(trainable_params, lr=pretrain_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc33204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# --- 事前学習ループ ---\n",
    "training_time = 0\n",
    "for epoch in range(pretrain_epochs):\n",
    "    pretrain_model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    for eeg, image in tqdm(pretrain_loader, desc=f\"Pre-training Epoch {epoch+1}\"):\n",
    "        eeg, image = eeg.to(device), image.to(device)\n",
    "        \n",
    "        eeg_embeds, img_embeds = pretrain_model(eeg, image)\n",
    "        \n",
    "        loss = contrastive_loss(eeg_embeds, img_embeds)\n",
    "        \n",
    "        pretrain_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        pretrain_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(pretrain_loader)\n",
    "    print(f\"Epoch {epoch+1}/{pretrain_epochs}, Contrastive Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} training time: {time.time() - start_time:.2f} seconds\")\n",
    "    training_time += time.time() - start_time\n",
    "\n",
    "# --- 事前学習済みの脳波エンコーダの重みを保存 ---\n",
    "torch.save(pretrain_model.eeg_encoder.state_dict(), \"pretrained_eeg_encoder.pt\")\n",
    "print(\"\\nPre-trained EEG encoder weights saved to pretrained_eeg_encoder.pt\")\n",
    "print(f\"Total training time for pre-training: {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c63ecb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClassfier(nn.Module):\n",
    "    \"\"\"事前学習済みのエンコーダーに分類ヘッドを追加するモデル\"\"\"\n",
    "    def __init__(self, pretrained_encoder: nn.Module , num_classes: int = 5):\n",
    "        super().__init__()\n",
    "        self.encoder = pretrained_encoder\n",
    "\n",
    "        embedding_dim = self.encoder.fc.out_features\n",
    "        self.classifier_head = nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.classifier_head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7759d49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained EEG encoder weights loaded successfully.\n",
      "Fine-tuning model parameters set with different learning rates for encoder and classifier head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/pftl20sn6t563rd47ngx0_f40000gn/T/ipykernel_52840/2222896149.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eeg_encoder_for_finetune.load_state_dict(torch.load(\"pretrained_eeg_encoder.pt\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# ベースとなる分類器の初期化\n",
    "eeg_encoder_for_finetune = ConvRNNEncoder(in_channels=17, embedding_dim=256)\n",
    "\n",
    "try:\n",
    "    eeg_encoder_for_finetune.load_state_dict(torch.load(\"pretrained_eeg_encoder.pt\", map_location=device))\n",
    "    print(\"Pre-trained EEG encoder weights loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Pre-trained EEG encoder weights not found. Please run the pre-training step first.\")\n",
    "\n",
    "finetune_model = FinalClassfier(eeg_encoder_for_finetune, num_classes=5).to(device)\n",
    "\n",
    "params_to_update =  [\n",
    "    {\"params\": finetune_model.encoder.parameters(), 'lr': 1e-5},\n",
    "    {\"params\": finetune_model.classifier_head.parameters(), 'lr': 1e-3}\n",
    "]\n",
    "finetune_optimizer = torch.optim.Adam(params_to_update)\n",
    "print(\"Fine-tuning model parameters set with different learning rates for encoder and classifier head.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2463a270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN SET] EEG: torch.Size([118800, 17, 100]), Subject Indices: torch.Size([118800]), Labels: torch.Size([118800])\n",
      "[VAL SET] EEG: torch.Size([59400, 17, 100]), Subject Indices: torch.Size([59400]), Labels: torch.Size([59400])\n"
     ]
    }
   ],
   "source": [
    "finetune_batch_size = 512\n",
    "finetune_epochs = 50\n",
    "\n",
    "train_set = ThingsEEGDataset(\"train\")\n",
    "train_loader = DataLoader(train_set, batch_size=finetune_batch_size, shuffle=True)\n",
    "\n",
    "val_set = ThingsEEGDataset(\"val\")\n",
    "val_loader = DataLoader(val_set, batch_size=finetune_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "080fcac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価指標（正解率）\n",
    "def accuracy(y_pred, y):\n",
    "    return (y_pred.argmax(dim=-1) == y).float().mean()\n",
    "\n",
    "# TensorBoardの準備\n",
    "writer = SummaryWriter(\"runs/eeg_fine_tune_experiment_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val_acc = 0.0\n",
    "\n",
    "for epoch in range(finetune_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{finetune_epochs}\")\n",
    "    \n",
    "    # ------ 訓練 ------\n",
    "    finetune_model.train()\n",
    "    train_loss_list, train_acc_list = [], []\n",
    "    for X, y, subject_idxs in tqdm(train_loader, desc=\"Train\"):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = finetune_model(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        finetune_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "        \n",
    "        train_loss_list.append(loss.item())\n",
    "        train_acc_list.append(accuracy(y_pred, y).item())\n",
    "\n",
    "    avg_train_loss = np.mean(train_loss_list)\n",
    "    avg_train_acc = np.mean(train_acc_list)\n",
    "\n",
    "    # ------ 検証 ------\n",
    "    finetune_model.eval()\n",
    "    val_loss_list, val_acc_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y, subject_idxs in tqdm(val_loader, desc=\"Validation\"):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = finetune_model(X)\n",
    "            loss = F.cross_entropy(y_pred, y) # 損失関数は要確認\n",
    "            val_loss_list.append(loss.item())\n",
    "            val_acc_list.append(accuracy(y_pred, y).item())\n",
    "            \n",
    "    avg_val_loss = np.mean(val_loss_list)\n",
    "    avg_val_acc = np.mean(val_acc_list)\n",
    "\n",
    "    # --- ログ表示 & TensorBoard記録 ---\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {avg_val_loss:.4f}, Val   Acc: {avg_val_acc:.4f}\")\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", avg_train_acc, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", avg_val_acc, epoch)\n",
    "\n",
    "    # --- モデルのベストパラメータを保存 ---\n",
    "    if avg_val_acc > max_val_acc:\n",
    "        cprint(f\"  New best validation accuracy! Saving model to finetune_model_best.pt\", \"cyan\")\n",
    "        torch.save(finetune_model.state_dict(), \"finetune_model_best.pt\")\n",
    "        max_val_acc = avg_val_acc\n",
    "\n",
    "writer.close()\n",
    "print(\"\\nTraining finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0aafc28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST SET] EEG: torch.Size([59400, 17, 100]), Subject Indices: torch.Size([59400])\n",
      "Best model weights loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/pftl20sn6t563rd47ngx0_f40000gn/T/ipykernel_52840/4179016361.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"finetune_model_best.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c40cf6eea9644f5aa4327b66b1d178e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file 'submission.npy' saved with shape: (59400, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = finetune_batch_size\n",
    "\n",
    "test_set = ThingsEEGDataset(\"test\")\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "encoder_base = ConvRNNEncoder(in_channels=test_set.num_channels, embedding_dim=256)\n",
    "model = FinalClassfier(encoder_base, num_classes=test_set.num_classes).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"finetune_model_best.pt\", map_location=device))\n",
    "print(\"Best model weights loaded.\")\n",
    "\n",
    "preds = []\n",
    "model.eval()\n",
    "with torch.no_grad(): \n",
    "    for X, subject_idxs in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "        X = X.to(device)\n",
    "        y_pred = model(X)\n",
    "        preds.append(y_pred.detach().cpu()) # 予測結果をCPUに移してから保存\n",
    "\n",
    "# 全ての予測結果を一つのテンサーに結合し、NumPy配列に変換\n",
    "preds = torch.cat(preds, dim=0).numpy()\n",
    "\n",
    "# 提出用ファイルとして保存\n",
    "np.save(f\"submission_finetuned.npy\", preds)\n",
    "print(f\"\\nSubmission file 'submission.npy' saved with shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
